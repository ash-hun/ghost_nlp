![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F57034e38-1e43-43c0-9399-31d3f1f67620%2FUntitled.png?table=block&id=ed1c95ea-f7b6-47c1-b587-6d349feccac6&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)  
  
**íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)** ëŠ” 2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•˜ì—¬ NIPSì— ë“±ì¬ëœ ë…¼ë¬¸ì¸ **â€œAttention is all you needâ€** ì—ì„œ ë‚˜ì˜¨ ëª¨ë¸ë¡œ ê¸°ì¡´ì˜ seq2seqì˜ êµ¬ì¡°ì¸ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ë”°ë¥´ë©´ì„œë„, ë…¼ë¬¸ì˜ íƒ€ì´í‹€ì²˜ëŸ¼ ì–´í…ì…˜(Attention)ë§Œìœ¼ë¡œ êµ¬í˜„í•œ ëª¨ë¸ì´ë‹¤. ì´ ëª¨ë¸ì€ *RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ìŒì—ë„ ë²ˆì—­ ì„±ëŠ¥ì—ì„œ RNNë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.*

### ğŸª§ ***Road Map***
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
**1.** Overview
**2.** Positional Encoding
**3.** Self-Attention (+ Multi-Head Attention)
**4.** Residual Learning
**5.** Add + Norm
**6.** Attention in Encoder and Decoder
**7.** Position-wise Feedforward Networks
**8.** Output Probabilities
**9.** Transformer : Attention Is All New Need


# â… . Overview

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F90636fb7-bfd7-49eb-a357-4109eb80277f%2FUntitled.png?table=block&id=81eeea3a-ec7c-4bb2-bef7-f0128271537e&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

**íŠ¸ëœìŠ¤í¬ë¨¸** ëŠ” RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ê¸°ì¡´ì˜ Seq2Seqì²˜ëŸ¼ ì¸ì½”ë”ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ê³ , ë””ì½”ë”ì—ì„œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³  ìˆë‹¤. í¬ê²Œ ì¸ì½”ë”ì—ì„œ ë””ì½”ë”ë¡œ íë¦„ì´ ì´ì–´ì§€ë©°, ë°ì´í„°ê°€ ì…ë ¥ë˜ê³  ì¶œë ¥ê°’ì´ ë‚˜ì˜¤ëŠ” íë¦„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ê°œë…ì„ ì•Œì•„ë³´ê³  ë§ˆì§€ë§‰ì— ì „ì²´ì •ë¦¬ë¥¼ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ê² ë‹¤.

![FIG 00. Transformerì˜ êµ¬ì¡°](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbe5e61c6-ea1a-4f1d-994c-877de7f20b8f%2FUntitled.png?table=block&id=2463764b-c400-460d-b4f6-eba7b7d609eb&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ìœ„ì™€ ê°™ì€ **Transformer**ì˜ êµ¬ì¡°ë¥¼ ë³´ë©´ ì•Œê² ì§€ë§Œ ì´ì „ê¹Œì§€ ë°°ì› ë˜ Seq2Seqë‚˜ Attention Mechanismì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ì—ˆë˜ ìƒì†Œí•œ ê°œë…ì´ ë§ì´ ë“±ì¥í•œë‹¤. ê° ë¸”ë¡ì˜ í‘œê¸°ë‚˜ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ë©´ì„œ ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ì´ ë“±ì¥í•  ê²ƒì´ë‹¤.

- Positional Encoding
- Self-Attention
- Multi-Head Attention
- Residual Learning & Residual Connection
- Feed Forward
- Encoder & Decoder

ì´ëŸ¬í•œ ê° ê°œë…ì„ ë¨¼ì € ì•Œì•„ë³¸ë’¤ ì „ì²´ì ì¸ **Transformer**ì˜ íë¦„ì„ ì‚´í´ë³´ì. 

# â…¡. Positional Encoding

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê¸°ì¡´ì˜ ë°©ì‹ê³¼ëŠ” íŒ¨ëŸ¬ë‹¤ì„ì´ ë‹¤ë¥´ë‹¤.

Attention Mechanismì„ ì‚¬ìš©í•˜ì§€ë§Œ ê¸°ì´ˆì ì¸ ëª¨ë¸ì¸ RNNê³¼ CNNì´ ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ê¸°ë•Œë¬¸ì— **ì„ë² ë”©ëœ ì…ë ¥ê°’ì˜ ìœ„ì¹˜ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤.** **ì¦‰, ë¬¸ì¥ ë‚´ ê°ê°ì˜ ë‹¨ì–´ì— ëŒ€í•œ ìˆœì„œì •ë³´ë¥¼ ì£¼ê¸° ìœ„í•´ Positional Encodingì´ë¼ëŠ” ê¸°ë²•ì„ ì‚¬ìš©**í•˜ì—¬ ì „ë‹¬í•´ì¤€ë‹¤. í–¥í›„ BERTì™€ ê°™ì€ ëª¨ë¸ì—ì„œë„ ì±„íƒí•œ ì¤‘ìš”í•œ ê°œë…ì´ë‹¤.

ìì„¸íˆ ì•Œì•„ë³´ì.

## (1). Traditional Embedding

ìš°ë¦¬ê°€ ì–´ë–¤ ë‹¨ì–´ì •ë³´ë¥¼ ë„¤íŠ¸ì›Œí¬ì— ë„£ê¸° ìœ„í•´ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì„ë² ë”© ê³¼ì •ì„ ê±°ì¹œë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9bd5d08c-4df5-4746-a75e-3d6f841bda54%2FUntitled.png?table=block&id=8b1556bf-1460-45af-9c57-cfeaecef9b45&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ë§¨ ì²˜ìŒ ì…ë ¥ ì°¨ì›ì€ íŠ¹ì • ì–¸ì–´ì—ì„œ ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ì™€ ê°™ê³  ë™ì‹œì— ê°ê°ì˜ ì •ë³´ë“¤ì€ ì›í•« ì¸ì½”ë”© í˜•íƒœë¡œ í‘œí˜„ì´ ë˜ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì— ë„£ì„ ë•Œ **Embedding**ì„ ê±°ì³ ë³´ë‹¤ ì ì€ ì°¨ì›ì˜ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ë„£ëŠ”ë‹¤. 

## (2). Positional Encoding

ë§Œì•½ **Transformer**ê°€ ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ê³¼ ê°™ì´ RNNê¸°ë°˜ì˜ ëª¨ë¸ì„ í™œìš©í–ˆì—ˆë”ë¼ë©´ ì´ ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ ***ê°ê°ì˜ ë‹¨ì–´ê°€ RNNì— ë“¤ì–´ê°ˆ ë•Œ ìˆœì„œì— ë§ê²Œ ì…ë ¥ë˜ì–´ ê° ì…€ì˜ íˆë“ ìŠ¤í…Œì´íŠ¸ëŠ” ìë™ì ìœ¼ë¡œ ìˆœì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.*** 

ë‹¤ë§Œ, **Transformer**ì™€ ê°™ì´ RNNê¸°ë°˜ì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ **íŠ¹ì •í•œ ë‹¨ì–´ê°€ ì–´ë– í•œ ë‹¨ìœ„ ì•(í˜¹ì€ ë’¤)ì— ìœ„ì¹˜í•˜ëŠ”ì§€ ìœ„ì¹˜(=ìˆœì„œ)ì •ë³´ë¥¼ í¬í•¨í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•´ì•¼ í•˜ê³ ** ì´ë¥¼ ìœ„í•´  **Postional Encoding**ì„ ì‚¬ìš©í•œë‹¤.

![Fig-01. Encoder Input Embedding Parts : â€˜Transformer : Attention is All You Needâ€™, NIPS, 2017](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0d2bcbe2-21e2-41ac-9bf5-ae8e54a71043%2FUntitled.png?table=block&id=c42670c2-d252-4560-8c1e-3cfcf250ef7e&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

**Input Embedding** ê°’ê³¼ **Positional Encoding**ëœ ê°’ì„ ***ê°ê° Element Wiseë¡œ ë”í•´*** ê° ë‹¨ì–´ì— ëŒ€í•œ ìœ„ì¹˜ì •ë³´ë¥¼ ë„¤íŠ¸ì›Œí¬ê°€ ì•Œ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë‚˜ì˜¨ ê°’ì„ Attentionì— ë„£ì–´ì£¼ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. 
ì¡°ê¸ˆ ë” ê¹Šê²Œ ì•Œì•„ë³´ì.

### 1. Input Embedding

**Input Embedding**ì€ Inputì— ì…ë ¥ëœ ë°ì´í„°ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°”ê¾¸ëŠ” ì‘ì—…ì´ë‹¤. 
ìš°ë¦¬ê°€ ì˜ ì•Œê³  ìˆë“¯ Inputsìœ¼ë¡œ ë“¤ì–´ì˜¨ Corpusê°€ Integer Encodingì„ í•œ ë’¤ ê·¸ ê°’ë“¤ì„ ê°€ì ¸ê°€ê²Œ ëœë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb6647f04-14fe-47f2-9df7-d92120c961e2%2FUntitled.png?table=block&id=707fa060-8b46-425f-a7d6-a9290b58f3db&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ê°€ë ¹, â€œThis is my carâ€ë¼ëŠ” ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ê°ê°ì˜ ë‹¨ì–´ëŠ” ê·¸ì— ìƒì‘í•˜ëŠ” ì¸ë±ìŠ¤ ê°’ì— ë§¤ì¹­ì´ ë˜ê³ , ì´ ê°’ë“¤ì€ Input Embeddingì— ì „ë‹¬ë˜ëŠ” ê²ƒì´ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa2228366-64fa-4fb8-a3f7-44dd6d63d395%2FUntitled.png?table=block&id=3d924d0f-c6a0-43d6-9836-ecacb1972885&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì´ë•Œ ê°ê°ì˜ ë‹¨ì–´ ì¸ë±ìŠ¤ë“¤ì€ ì €ë§ˆë‹¤ ë‹¤ë¥¸ ë²¡í„°ê°’ì„ ì§€ë‹ˆê³  ìˆëŠ”ë° (ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ $d_{model}$ì´ë¼ê³  í•˜ê³  512ë¥¼ ì‚¬ìš©í–ˆë‹¤.) ê°ê°ì˜ ë²¡í„° ì°¨ì›ì€ í•´ë‹¹ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ê°€ì§€ë©° ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ì •ë³´ê°€ ìœ ì‚¬í•  ìˆ˜ë¡ ì„ë² ë”©ëœ ë²¡í„°ê³µê°„ì—ì„œì˜ ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ ê²ƒì´ë‹¤. (ì¦‰, ë²¡í„°ê³µê°„ì—ì„œì˜ ê±°ë¦¬ê°€ ê°€ê¹Œìš´ ë‘ ë‹¨ì–´ê°€ ìœ ì‚¬í•¨ì„ ì˜ë¯¸í•œë‹¤.)

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3cd1fbb7-5b23-496f-97cb-246ae73c440f%2FUntitled.png?table=block&id=00489cc7-b4e3-488b-8f86-44aafbd15076&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

Embedding LayerëŠ” Input indexê°’ë“¤ì„ ë°›ì•„ì„œ ê°ê°ì˜ ë‹¨ì–´ ì„ë² ë”© ë²¡í„°ê°’ìœ¼ë¡œ ë°”ê¿”ì£¼ê³  ì´ë ‡ê²Œ ë‚˜ì˜¨ ê°’ë“¤ì— Positional Encodingì˜ ë²¡í„°ê°’ì„ ë”í•˜ëŠ” ì—°ì‚°ì„ í•˜ê²Œ ëœë‹¤.

### 2. ë‹¨ì–´ì˜ ìœ„ì¹˜ì •ë³´

**Transformer** êµ¬ì¡°ì—ì„  ë‹¨ì–´ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ì„ë² ë”©ëœ ë²¡í„°ê°’ì— element wiseë¡œ ë”í•´ì¤€ë‹¤ê³  í–ˆë‹¤. 
ì´ì— ëŒ€í•´ ì•Œì•„ë³´ê¸°ì „ **ìœ„ì¹˜ì •ë³´ëŠ” ì™œ ì¤‘ìš”í•œì§€**ì— ëŒ€í•´ ì•Œì•„ë³´ì. ì•„ë˜ 2ë¬¸ì¥ì„ ë³´ì.

- Although I did not get 95 in last TOEFL, I could get in the Ph.D program.
- Although I did get 95 in last TOEFL, I could not get in the Ph.D program.

ìœ„ 2ê°œë¬¸ì¥ì„ í•´ì„œí•´ë³´ë©´, 
1ë²ˆ ë¬¸ì¥ì€ â€œì§€ë‚œ í† í”Œì‹œí—˜ì—ì„œ 95ì ì„ ëª»ë°›ì•˜ì§€ë§Œ, ë°•ì‚¬ê³¼ì •ì— ì…í•™í•  ìˆ˜ ìˆì—ˆë‹¤.â€ì´ê³ , 
2ë²ˆ ë¬¸ì¥ì€ â€œì§€ë‚œ í† í”Œì‹œí—˜ì—ì„œ 95ì ì„ ë°›ì•˜ì§€ë§Œ, ë°•ì‚¬ê³¼ì •ì— ì…í•™í•˜ì§€ ëª»í–ˆë‹¤.â€ë¡œ í•´ì„ì´ ëœë‹¤. 
***ì¦‰, notì˜ ìœ„ì¹˜ì— ë”°ë¼ ë¬¸ì¥ì˜ ëœ»ì´ ì™„ì „íˆ ë‹¬ë¼ì§€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.***
ê·¸ë˜ì„œ ì„ë² ë”© ëœ ë²¡í„°ê°’ì— ë‹¨ì–´ë“¤ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ë”í•´ì¤˜ì•¼ í•˜ëŠ”ë°, ì´ë•Œ ì§€ì¼œì•¼ í•  ê·œì¹™ì´ 2ê°€ì§€ê°€ ìˆë‹¤.

1. **ëª¨ë“  ìœ„ì¹˜ê°’ì€ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë‚˜ Inputì— ê´€ê³„ì—†ì´ ë™ì¼í•œ ì‹ë³„ìë¥¼ ê°€ì ¸ì•¼ í•œë‹¤.** 
ì¦‰, ê° ìœ„ì¹˜ì— ë”°ë¥¸ ë‹¨ì–´ê°€ ë°”ë€Œë”ë¼ë„ ìœ„ì¹˜ ì„ë² ë”©ì€ ë™ì¼í•˜ê²Œ ìœ ì§€ë  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
    
    ![ë‹¨ì–´ì˜ ë²¡í„°ê°’ì´ ë³€í•´ë„ ìœ„ì¹˜ ì„ë² ë”© ê°’ì€ ë³€í•¨ì´ ì—†ë‹¤!](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff97eee41-4574-48c5-822d-9d3b03b79379%2FUntitled.png?table=block&id=14d5e143-76e6-435e-89ac-b1a8aea01d53&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
    
    ë‹¨ì–´ì˜ ë²¡í„°ê°’ì´ ë³€í•´ë„ ìœ„ì¹˜ ì„ë² ë”© ê°’ì€ ë³€í•¨ì´ ì—†ë‹¤!
    
2. **ëª¨ë“  ìœ„ì¹˜ ì„ë² ë”©ê°’ì€ ë„ˆë¬´ í¬ë©´ ì•ˆëœë‹¤.** 
ë²¡í„°ê³µê°„ì† ì„ë² ë”©ëœ ë‹¨ì–´ë“¤ì´ ìœ„ì¹˜ê°’ì„ ë”í•´ì„œ ìˆœì„œë¥¼ ì•Œê²Œ ë˜ëŠ”ë°, ì´ë•Œ ìœ„ì¹˜ ì„ë² ë”©ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ë²„ë¦¬ë©´ ë‹¨ì–´ ê°„ì˜ ìƒê´€ê´€ê³„ ë° ì˜ë¯¸ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ì˜ë¯¸ì •ë³´ ê°’ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì•„ì§€ê²Œ ë˜ê³ , Attention Layerë¥¼ í†µê³¼í•  ë•Œ ì œëŒ€ë¡œ í•™ìŠµì´ ì•ˆë  ìˆ˜ë„ ìˆë‹¤.
    
    ![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F29ee43a5-ee20-4fc5-9446-252787e81a09%2FUntitled.png?table=block&id=c9a05931-7dd7-4137-ad97-e152d5f6f09f&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
    

### 3. ìœ„ì¹˜ ë²¡í„°ë¥¼ ì–»ëŠ” ë°©ë²•

ìœ„ì¹˜ ë²¡í„°ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” **â€˜ì£¼ê¸°í•¨ìˆ˜â€™**ë¥¼ ì‚¬ìš©í•œë‹¤.

ì•ì„œ ë§í–ˆë˜ ìœ„ì¹˜ ì„ë² ë”©ì˜ ê°’ì´ ë‹¨ì–´ì— ê´€ê³„ì—†ì´ ë™ì¼í•œ ì‹ë³„ìë¥¼ ê°€ì ¸ì•¼í•œë‹¤ëŠ” ì ê³¼ ê·¸ ê°’ì˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì•ˆë˜ëŠ” ì ì„ ê³ ë ¤í–ˆì„ ë•Œ ì£¼ê¸°í•¨ìˆ˜ì¸ ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ê°€ ì ì ˆí•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.

Sine & Cosine í•¨ìˆ˜ëŠ” -1ê³¼ 1ì‚¬ì´ë¥¼ ë°˜ë³µí•˜ëŠ” ì£¼ê¸°í•¨ìˆ˜ë¡œ 1ì„ ì´ˆê³¼í•˜ì§€ ì•Šê³  -1ë¯¸ë§Œìœ¼ë¡œ ë–¨ì–´ì§€ì§€ ì•Šì•„ ê°’ì˜ ë²”ìœ„ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•ŠëŠ” ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤.

**ê°™ì€ ìœ„ì¹˜ì˜ ë‹¨ì–´(=í† í°)ëŠ” í•­ìƒ ê°™ì€ ìœ„ì¹˜ ë²¡í„°ê°’ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ê³ **, ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì˜ í† í°ì€ ìœ„ì¹˜ ë²¡í„°ê°’ì´ ì„œë¡œ ë‹¬ë¼ì•¼ í•œë‹¤. ì—¬ê¸°ì„œ Sine & Cosine í•¨ìˆ˜ëŠ” ì£¼ê¸°í•¨ìˆ˜ì˜ íŠ¹ì§•ë•Œë¬¸ì— ìœ„ì¹˜ ë²¡í„°ê°’ì´ ê²¹ì¹  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbaccb1a2-51f1-4e17-b985-99acb86a3b65%2FUntitled.png?table=block&id=9be9b1f7-e374-4fec-801c-5a8cbfa01e34&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ìœ„ ê·¸ë¦¼ë§Œ ë³´ë©´ $p_0$ì™€ $p_8$ì˜ ìœ„ì¹˜ ë²¡í„°ê°’ì´ ë™ì¼í•˜ë‹¤. ìš°ë¦¬ê°€ í—·ê°ˆë¦¬ë©´ ì•ˆë˜ëŠ” ì ì´ Positional Encodingê°’ì€ ë²¡í„°ê°’ìœ¼ë¡œ **ì°¨ì›ì„ ì§€ë‹Œë‹¤**ëŠ” ì ì´ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2784a3f2-ccbb-4466-aa21-3ea316150611%2FUntitled.png?table=block&id=b37c9b95-b254-42a6-be74-71295cab211a&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì¦‰, ìœ„ì¹˜ ë²¡í„°ê°’ì´ ê°™ì•„ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Sineê³¼ Cosineí•¨ìˆ˜ë¥¼ ë™ì‹œì— ì‚¬ìš©í•œë‹¤. 
ë§Œì•½ ê·¸ë¦¼ê³¼ ê°™ì´ í•˜ë‚˜ì˜ ìœ„ì¹˜ ë²¡í„°ê°€ 4ê°œì˜ ì°¨ì›ìœ¼ë¡œ í‘œí˜„ë˜ë©´ ê° ìš”ì†ŒëŠ” ì„œë¡œ ë‹¤ë¥¸ 4ê°œì˜ ì£¼ê¸°ë¥¼ ê°€ì§€ê²Œ ë˜ë¯€ë¡œ ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ”ë‹¤.

ê·¸ëŸ¼ì—ë„ ê° ì°¨ì›ì˜ ë²¡í„°ê°’ë“¤ì˜ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ ë²¡í„° ê°„ì˜ ìœ„ì¹˜ ì •ë³´ ì°¨ì´ê°€ ë¯¸ë¯¸í•  ê²ƒì´ë‹¤. ì´ ê²½ìš° ì£¼ê¸°í•¨ìˆ˜ì˜ Frequencyë¥¼ ì´ì „ ì£¼ê¸°í•¨ìˆ˜ë³´ë‹¤ í¬ê²Œ ì£¼ë©´ë˜ê³ , ë§ˆì§€ë§‰ ì°¨ì›ì˜ ë²¡í„°ê°’ì´ ì±„ì›Œì§ˆ ë•Œê¹Œì§€ ì„œë¡œ ë‹¤ë¥¸ frequencyë¥¼ ê°€ì§„ Sine & Cosineì„ ë²ˆê°ˆì•„ê°€ë©° ê³„ì‚°í•˜ë‹¤ë³´ë©´ ê²°ê³¼ì ìœ¼ë¡œ ì¶©ë¶„íˆ ë‹¤ë¥¸ Positional Encoding ê°’ì„ ì§€ë‹ˆê²Œ ëœë‹¤. ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

â€» $pos$ëŠ” positionì„ ë§í•˜ë©°, $i$ëŠ” ì°¨ì›ì„ ì˜ë¯¸í•œë‹¤.

$$
PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})
$$

### 4. Input Embeddingê³¼ Positional Encoding ê°„ì˜ ì—°ì‚°

Seq2Seq with Attentionì„ ë³´ë©´ Attention Mechanism (ì •í™•íˆëŠ” dot product attention)ì˜ í›„ë°˜ë¶€ ê³„ì‚° ë¶€ë¶„ì—ì„œ Concatenateë¥¼ ì ìš©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  ì—¬ê¸°ì—ì„œëŠ” ì™œ Concatenateê°€ ì•„ë‹Œ Summationì„ ì‚¬ìš©í•œ ê²ƒì¼ê¹Œ?

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F848ee365-af81-4ed1-88b0-88755af8ce78%2FUntitled.png?table=block&id=8cbd55d3-d101-407e-a0cf-df6e600e7811&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ìœ„ ê·¸ë¦¼ì´ summation ëŒ€ì‹  concatenateë¥¼ ì‚¬ìš©í•œ ê²½ìš°ì´ë‹¤.

**Concatenate**ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¨ì–´ ì˜ë¯¸ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ë‹¨ì–´ ë²¡í„° ë’¤ì— ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” Positional Embeddingì´ ì—°ê²°ëœë‹¤. ì´ ê²½ìš° ë‹¨ì–´ì˜ ì˜ë¯¸ ì •ë³´ëŠ” ìì²´ ì°¨ì›ê³µê°„ì„ ê°€ì§€ê²Œ ë˜ë©°, ìœ„ì¹˜ ì •ë³´ ì—­ì‹œ ë§ˆì°¬ê°€ì§€ì´ë‹¤. ì¦‰, ì§êµì„±ì§ˆ(orthogonal)ì— ì˜í•´ ë‘˜ì€ ì„œë¡œ ì „í˜€ ê´€ê³„ì—†ëŠ” ê³µê°„ì— ìˆê²Œ ëœë‹¤. 

    ğŸ”¥ ìœ„ pragraphì— ëŒ€í•œ ë‚´ìš©ì„ ì´í•´í•˜ê¸° ì–´ë ¤ì›Œì„œ ìë¬¸ì„ êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ì„ ì–»ì§€ëª»í–ˆë‹¤.. 
    ì‹œê°„ì„ ë“¤ì—¬ ì¶”ê°€ì ì¸ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì°¾ì•„ ì•Œê²Œëœ ë‚´ìš©ì„ ì •ë¦¬í•œë‹¤.

    ê²°ë¡ ë¶€í„° ë§í•˜ë©´, Element Wise Summation(ì´í•˜ Addë¼ ì§€ì¹­í•¨)ê³¼ Concatenate(ì´í•˜ Conì´ë¼ ì§€ì¹­í•¨)ëŠ” ë³¸ì¸ì´ êµ¬ì„±í•œ ì‹ ê²½ë§ êµ¬ì¡°ì™€ í•˜ê³ ìí•˜ëŠ” í…ŒìŠ¤í¬ì˜ ëª©ì ì— ë§ê²Œ ì ì ˆí•˜ê²Œ ì„ íƒí•´ì•¼ í•œë‹¤ê³  í•œë‹¤.  (ë¹„ë¡, ë©”ëª¨ë¦¬ ì‚¬ìš©ê´€ì ì—ì„œëŠ” Element Wise Summationì´ Concatenateë³´ë‹¤ ë” ì¢‹ì§€ë§Œ ë§ì´ë‹¤!)

    í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ì²¨ë¶€í•˜ê² ë‹¤.
    Aê°€ 3000ì›, Bê°€ 2000ì›, Cê°€ 5000ì›ì„ ê°€ì§€ê³  ìˆë‹¤ê³  í•˜ì.
    ì—¬ê¸°ì„œ ì´í•©ì„ êµ¬í•  ë•Œ Addì™€ Concatenateì˜ ë°©ì‹ì°¨ì´ë¥¼ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
    - Add : ì „ì²´ ì´í•©ì´ 10000ì›ì´ë‹¤.
    - Con : Aê°€ 3000ì›, Bê°€ 2000ì›, Cê°€ 5000ì›ì„ ê°€ì ¸ì„œ ì „ì²´ ì´í•©ì´ 10000ì›ì´ë‹¤.

    ë§Œì•½ 7000ì›ì§œë¦¬ ë¬¼ê±´ì„ ì‚´ ë•Œ Addì™€ Con 2ê°€ì§€ ë°©ì‹ì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€?
    - Add : ì „ì²´ ì´í•©ë§Œì„ ì•Œê³ ìˆìœ¼ë¯€ë¡œ 7000ì›ì§œë¦¬ ë¬¼ê±´ì„ ì‚´ ìˆ˜ìˆëŠ”ì§€ ì—†ëŠ”ì§€ íŒë‹¨ì´ ë¹ ë¥´ë‹¤.
    - Con : ì „ì²´ ì´í•© ì¤‘ ëˆ„ê°€ ì–¼ë§ˆë¥¼ ëƒˆëŠ”ì§€ ì•Œê³ ìˆìœ¼ë¯€ë¡œ 7000ì›ì§œë¦¬ ë¬¼ê±´ì„ ì‚¬ê¸°ìœ„í•´ ëˆ„ê°€ ì–¼ë§ˆë¥¼ 
                    ë‚´ì•¼í•  ì§€ íŒë‹¨ì´ ë¹ ë¥´ë‹¤.

    ì¦‰, ì´ë¥¼ ë²¡í„°ì˜ ê´€ì ì—ì„œ ë°”ë¼ë³´ê²Œë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆê² ë‹¤.
    - Add : ë”í•´ì§€ëŠ” í•©ê³„ê°’ì´ í•˜ë‚˜ì˜ ë¬¶ìŒìœ¼ë¡œ ë³´ê³  ì”ì°¨ë¡œ ì¸ì‹í•œë‹¤.(ex, residual learning)
    - Con : ì¶”ì¶œí•œ featureì˜ ìœ„ì¹˜(ìˆœì„œ)ê°’ì„ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ê³ ì í•œë‹¤. ë‘ ê°€ì§€ featureë“¤ì´ ë°€ì ‘í•œ ê´€ë ¨ì„±ì„ ê°€ì§€ì§€ ì•Šì„ ë•Œ ì‚¬ìš©í•˜ëŠ”ê²Œ ë” ì¢‹ë‹¤.

    ì–´ë– í•œ ë°©ì‹ì„ ì ìš©í•´ë„ ë¬´ë°©í•˜ì§€ë§Œ, ì ì ˆí•œ ìƒí™©ì— ë”°ë¼ ì„ íƒí•´ì•¼ í•˜ê³ , 
    Transformerê°€ ë“±ì¬ëœ ì‹œì ì—ì„  ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ì„œ Add(=Element Wise Summation)ì„ ì„ íƒí•œê²ƒìœ¼ë¡œ ì¶”ì¸¡ëœë‹¤.

</aside>

**Concatenate**ëŠ” ì •ë³´ì˜ ì„ì„ì„ ë°©ì§€í•´ í˜¼ë€ì„ í”¼í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì§€ë§Œ ë©”ëª¨ë¦¬, íŒŒë¼ë¯¸í„°, ëŸ°íƒ€ì„ ë“±ì˜ costë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì´ì— ë°˜í•´, **Summation**ì€ ë‹¨ì–´ ì˜ë¯¸ì •ë³´ì™€ ìœ„ì¹˜ ì •ë³´ê°„ì˜ ê· í˜•ì´ ì˜ ë§ì¶°ì§€ëŠ”ë° ì •ë³´ê°€ ë’¤ì„ì´ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ëŠ” ì—†ë‹¤. ì¦‰, ëª¨ë¸ì´ ë§¤ìš° í¬ê³  GPUì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ (Cost ë¬¸ì œê°€ í•´ê²°ì´ ëœë‹¤ë©´) **Concatenate**ë¥¼ ì‚¬ìš©í•´ë„ ê´œì°®ë‹¤!

**Transformer** ë…¼ë¬¸ì€ NIPSì— 2017ë…„ ë“±ë¡ëœ ë…¼ë¬¸ìœ¼ë¡œ í˜„ì¬ì™€ ë‹¤ë¥´ê²Œ ê·¸ë‹¹ì‹œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•˜ì„ ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤. ê·¸ë ‡ê¸°ì— **Summation**ì„ ì„ íƒí•œ ê²ƒ ê°™ë‹¤.

**ê²°êµ­, Postional Encodingì„ ì ìš©í•¨ìœ¼ë¡œì¨ ë‹¤ëŸ‰ì˜ ë‹¨ì–´ ë²¡í„°ë“¤ì„ ë³‘ë ¬ì ìœ¼ë¡œ í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.**

## (3). ì½”ë“œ êµ¬í˜„

ì¶”í›„ **Transformer** êµ¬í˜„ì„ í•˜ë©´ì„œ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ê³  ì§€ê¸ˆì€ ì›ë¦¬ì— ì´ˆì ì„ ë§ì¶° ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•´ë³´ì.

```python
# Positional Encoding
import math
import matplotlib.pyplot as plt

n = 4 # ë‹¨ì–´(word)ì˜ ê°œìˆ˜
dim = 8 # ì„ë² ë”©(embedding) ì°¨ì› ìˆ˜

def get_angles(pos, i, dim):
    angles = 1/math.pow(10000,(2*(i//2))/dim)
    return pos * angles

def get_positional_encoding(pos, i, dim):
    if i%2 == 0:
        return math.sin(get_angles(pos, i, dim))
    return math.cos(get_angles(pos, i, dim))

result = [[0] * dim for _ in range(n)]

for i in range(n):
    for j in range(dim):
        result[i][j] = get_positional_encoding(i, j, dim)

plt.pcolormesh(result, cmap='Blues')
plt.show()
```

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe103e9ed-6c95-4fcb-a8cd-36fa38f0d276%2FUntitled.png?table=block&id=3d91958c-3e68-481d-8992-65fe4d63a9c5&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

# â…¢. Self-Attention

ê¸°ì¡´ì— ì–´í…ì…˜ ê¸°ë²•(Attention Mechanism)ì´ Machine Translatoion Taskì—ì„œ ì ìš©ë  ë•Œ ë²ˆì—­í•  ëŒ€ìƒì´ ë˜ëŠ” ë¬¸ì¥ì—ì„œ ì£¼ëª©í•´ì•¼ ë  íŠ¹ì • ë‹¨ì–´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ê³¼í•˜ì—¬ â€˜ì§‘ì¤‘â€™í•œë‹¤ë©´ ì–»ê³ ì í•˜ëŠ” ë‹µì„ ë¹ ë¥´ê²Œ ì–»ì„ ìˆ˜ ìˆìŒì„ ì´ì „ì— ê³µë¶€í–ˆì—ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ **Self-Attention**ì€ ë¬´ì—‡ì¼ê¹Œ? Attentionì•ì— â€˜Self-â€™ ê°€ ë¶™ì€ê²ƒë§Œ ë´ë„ ì•Œ ìˆ˜ ìˆë“¯ì´ ê°™ì€ ë¬¸ì¥ ë‚´ì—ì„œ ë‹¨ì–´ë“¤ê°„ì˜ ê´€ê³„, ì¦‰ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ ì–´í…ì…˜ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. Transformerì—ì„œëŠ” Self-Attentionì´ í•µì‹¬ì´ê³  Encoderì™€ Decoder 2ê°€ì§€ êµ¬ì¡° ëª¨ë‘ ì‚¬ìš©ëœë‹¤.

## (1). Query, Key, Value

**Attention**ì˜ ëª©í‘œëŠ” Valueë¥¼ í†µí•´ ê°€ì¤‘í•©ì„ ê³„ì‚°í•˜ëŠ” ê²ƒ(=ì´ë ‡ê²Œ êµ¬í•œ ê°’ì´ Attention Value)ì´ê³ , ê° Valueì˜ ê°€ì¤‘ì¹˜ëŠ” ì£¼ì–´ì§„ Queryì™€ Keyê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œê°€ì— ë”°ë¼ ê²°ì •ëœë‹¤. ê° ìš”ì†Œë“¤ì— ëŒ€í•œ ì˜ë¯¸ë¥¼ ì•Œì•„ë³´ì.

- **Query (ì¿¼ë¦¬)**
    - ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ê´€ë ¨ëœ ë¶€ë¶„ì„ ì°¾ìœ¼ë ¤ê³  í•˜ëŠ” ì •ë³´ì†ŒìŠ¤
- **Key (í‚¤)**
    - ê´€ê³„ì˜ ì—°ê´€ë„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ Queryì™€ ë¹„êµí•˜ê²Œ ë˜ëŠ” ë²¡í„°
- **Value (ë°¸ë¥˜)**
    - íŠ¹ì • keyì— í•´ë‹¹í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¡œ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë²¡í„°

## (2). Multi-Head Attention

**Transformer**ì—ì„œ ì–´í…ì…˜ì€ ì–´ë–»ê²Œ ì‚¬ìš©ë ê¹Œ? ì•„ë˜ ê·¸ë¦¼ì„ ì˜ ì‚´í´ë³´ì.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F04a48848-5145-43aa-bb12-3a592a39998e%2FUntitled.png?table=block&id=306b5446-871d-4613-aae6-2debff01a1a6&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

**Transformer**ì—ëŠ” **Multi-Head Attention**ì´ë¼ëŠ” ê¸°ë²•ì´ ì ìš©ë˜ì—ˆë‹¤.
ì¦‰, ê·¸ë¦¼ê³¼ ê°™ì´ ***Self-Attentionì„ ë³‘ë ¬ì ìœ¼ë¡œ hë²ˆ í•™ìŠµ***ì‹œì¼°ë‹¤ëŠ” ê²ƒì¸ë° ì´ì— ëŒ€í•´ ê¹Šê²Œ ì•Œì•„ë³´ê¸° ìœ„í•´ì„œëŠ” Self-Attentionì— ëŒ€í•´ ë¨¼ì € ê¹Šê²Œ ì•Œì•„ë³´ì•„ì•¼ í•œë‹¤. ê·¸ì „ì— Multi-Head Attentionì˜ êµ¬ì¡°ë¥¼ ë³´ë¼. Q, K, Vê°€ Linear Layerë¥¼ í†µê³¼í•˜ê³  ìˆì§€ ì•Šì€ê°€? ì´ì— ëŒ€í•´ ì ê¹ ì´ì•¼ê¸° í•˜ê² ë‹¤.

### (2)-1. Linear Layer

**Transformer**ì˜ ì „ì²´êµ¬ì¡°ë¥¼ ë³´ë©´ Input Embeddingëœ ë²¡í„°ì™€ **Postional Encoding**ëœ ê°’ì´ ê°ê° Element Wiseí•˜ê²Œ ë”í•´ì§€ê³ , ì´ ê°’ì´ Multi-Head Attention ë¸”ë¡ìœ¼ë¡œ ë“¤ì–´ì˜´ì„ ì•Œ ìˆ˜ ìˆë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F861f29ee-4da0-465a-83a9-c872185b4627%2FUntitled.png?table=block&id=a371e983-0884-4c78-aae5-22e1ae755952&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì¦‰, **Multi-Head Attention**ì´ ì¼ì–´ë‚˜ê¸° ì „ì— Linear Layerê°€ ìˆìœ¼ë¯€ë¡œ Linear Layerì— ì•ì„œë§í•œ ì…ë ¥ê°’ì´ ë“¤ì–´ì˜¨ë‹¤ëŠ” ê²ƒì´ë‹¤. ë‹¤ì‹œë§í•´ ê°ê°ì˜ Linear Layerì—ëŠ” ë™ì¼í•œ Embedding Vector + Positional Encoding ê°’ì´ ì…ë ¥ëœë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0adfe312-636a-4f1e-a093-2f7d3136e843%2FUntitled.png?table=block&id=38d61d62-1d69-43ae-a0f0-97d4b65406b1&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì—¬ê¸°ì„œ **Linear Layerë¥¼ íˆ¬ê³¼ì‹œí‚¤ëŠ” ì´ìœ **ë¥¼ 2ê°€ì§€ë¡œ ì„¤ëª…í•˜ìë©´

- Linear Layerê°€ ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ì—­í• ì„ í•˜ê³ 
- Linear Layerê°€ í–‰ë ¬ì´ë‚˜ ë²¡í„°ì˜ ì°¨ì›ì„ ë°”ê¾¸ëŠ” ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì •ë¦¬í•˜ë©´, **ì…ë ¥ê°’ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” Q, K, V ê°ê°ì˜ ì°¨ì›ì„ ì¤„ì—¬ì„œ ë³‘ë ¬ ì—°ì‚°ì— ì í•©í•œ êµ¬ì¡°ë¥¼ ë§Œë“¤ê³ ì í•˜ëŠ” ê²ƒ!**

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fffde5517-7c17-42ac-8c4d-e92ac5fc5c92%2FUntitled.png?table=block&id=7b35b081-1760-4bac-a169-a1492c624235&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

### (2)-2. Attention Score

Linear Layerë¥¼ í†µê³¼í•œ Q, K, VëŠ” **Scaled Dot-Product Attention** ë¸”ë¡ì„ í†µê³¼í•˜ê²Œ ë˜ëŠ”ë° 1ê°œì˜ Scaled Dot-Product Attention ë¸”ë¡ì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb39b6680-1c47-451a-93b4-4241534f81ad%2FUntitled.png?table=block&id=e972aa28-209b-4672-a5a5-c53f89f1dcf7&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì—¬ê¸°ë¶€í„°ëŠ” ìš°ë¦¬ê°€ ì•„ëŠ” ì–´í…ì…˜ê³¼ ê°™ë‹¤. Qì™€ K í–‰ë ¬ì˜ í–‰ë ¬ê³±(=MatMul, í–‰ë ¬ê°„ì˜ ìœ ì‚¬ë„ ì˜ë¯¸)ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì—¬ê¸°ì„œ **Self-Attention**ì´ ë“±ì¥í•œë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9fd08031-0f77-415f-b3b7-d31b828f71ca%2FUntitled.png?table=block&id=cf1bcb36-6d15-449e-9b67-8ae7fc05f58e&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

Self-Attentionì„ ì ê¹ ì§šê³  ë„˜ì–´ê°€ì. 
ì´ê²ƒë„ Attentionì˜ í•œ ì¢…ë¥˜ì´ê¸° ë•Œë¬¸ì— ì¿¼ë¦¬(Query), í‚¤(Key), ë°¸ë¥˜(Value)ì˜ 3ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. 
ë‹¤ë§Œ ì¼ë°˜ì ì¸ Attentionê³¼ ë‹¤ë¥¸ì ì´ ìˆë‹¤ë©´ í•œ ë¬¸ì¥ ì•ˆì—ì„œ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ë¬¸ë§¥ì  ê´€ê³„ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì´ë¼ëŠ” ì ì´ë‹¤.

ì•„ë˜ ìˆ˜ì‹ì²˜ëŸ¼ ì…ë ¥ ë²¡í„° ì‹œí€€ìŠ¤($\mathbf{X}$)ì— Query, Key, Valueë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í–‰ë ¬($\mathbf{W}$)ì„ ê°ê° ê³±í•œë‹¤. 

$$
\mathbf{Q} = \mathbf{X} \times \mathbf{W}_{Q}\\
\mathbf{K} = \mathbf{X} \times \mathbf{W}_{K}\\
\mathbf{V} = \mathbf{X} \times \mathbf{W}_{V}
$$

ìœ„ì™€ ê°™ì´ ë§Œë“  $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ê°’ì„ ê°€ì§€ê³  ì•„ë˜ ì…€í”„ ì–´í…ì…˜ ì •ì˜ì— ì…ê°í•˜ì—¬ ê³„ì‚°í•œë‹¤.

$$
Attention(\mathbf{Q},\mathbf{K},\mathbf{V}) = softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_K}})\mathbf{V}
$$

**ì¿¼ë¦¬ì™€ í‚¤ë¥¼ í–‰ë ¬ê³±** $(= \mathbf{Q}\mathbf{K}^T)$ í•œ ë’¤ 
í•´ë‹¹ í–‰ë ¬ì˜ **ëª¨ë“  ìš”ì†Œê°’ì„ í‚¤ ì°¨ì›ìˆ˜ì˜ ì œê³±ê·¼ ê°’ìœ¼ë¡œ ë‚˜ëˆ ** $(= \frac{1}{\sqrt{d_K}})$ ì£¼ê³ , 
ì´ í–‰ë ¬ì„ **í–‰(row)ë‹¨ìœ„ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax)ë¥¼ ì·¨í•´** $(= softmax(\;) )$ 
ìŠ¤ì½”ì–´ í–‰ë ¬ì„ ë§Œë“¤ì–´ ì¤€ë‹¤. 
ì´ë ‡ê²Œ ë§Œë“  ìŠ¤ì½”ì–´ í–‰ë ¬ì— **ë°¸ë¥˜ë¥¼** $(= \mathbf{V})$ **í–‰ë ¬ê³±** í•´ì£¼ì–´ **Self-Attention** ê³„ì‚°ì„ ë§ˆì¹œë‹¤. 

ì´ë•Œ softmax í•¨ìˆ˜ ì•ˆì— ë“¤ì–´ê°€ëŠ” ìˆ˜ì‹ $\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_K}}$ëŠ” ìš°ë¦¬ê°€ ì˜ ì•„ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³µì‹ì´ë‹¤. 
ëª¨ì–‘ì´ ì¢€ ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ì˜ë¯¸ë¥¼ ê³ ë ¤í•œë‹¤ë©´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë³„ë°˜ ë‹¤ë¥´ì§€ ì•ŠìŒì„ ì•Œ ê²ƒì´ë‹¤.
*ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë‘ ë²¡í„°ê°€ ìœ ì‚¬í•  ìˆ˜ë¡ ê°’ì´ 1ì— ê°€ê¹Œì›Œì§€ê³  ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ë¡ -1ì— ê°€ê¹Œì›Œì§€ëŠ” íŠ¹ì§•ì„ ì§€ë‹Œë‹¤.* 

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F23a8f3a7-0b94-43ed-9456-fb81410b17fc%2FUntitled.png?table=block&id=fe0233b8-b987-4dbb-8079-3cd06e85f9fd&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì¦‰, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë²¡í„°ì˜ ê³±ì„ ë‘ ë²¡í„°ì˜ L2 norm ê³±, ì¦‰ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤. 
ì´ë¥¼í†µí•´ í–‰ë ¬ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ìš°ë¦¬ê°€ ë³´ëŠ” Attentionì˜ ê²½ìš° í–‰ë ¬ ê°„ì˜ ê³±ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì°¨ì› ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•´ í–‰ë ¬ Bë¥¼ ì „ì¹˜í–‰ë ¬ë¡œ í˜•íƒœë³€í™˜í•˜ì—¬ ê³±í•´ì¤€ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9748e8d0-04b2-4bd5-bde0-42ee2dfc398e%2FUntitled.png?table=block&id=1d779280-c0a6-4d26-b383-412d9d8eda4f&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì´ë ‡ê²Œ êµ¬í•œ ê°’ì€ **Attention Score**ë¼ê³  í•˜ëŠ”ë° ê°€ë§Œíˆ ì‚´í´ë³´ë©´ ìê¸° ìì‹ ê³¼ ë§¤í•‘ë˜ëŠ” ê°’ì´ ê°€ì¥ í¬ê³  ê·¸ ë‹¤ìŒìœ¼ë¡œ ìœ ì‚¬í•œ ê°’ì´ í¬ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰ Scalingì´ í•„ìš”í•˜ê²Œ ëœê²ƒì´ë‹¤!

### (2)-3. Scaling & Softmax

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0aab34dc-1b5d-482c-9e14-ba53833251cd%2FUntitled.png?table=block&id=a1b25ac7-e4a0-4640-99dd-90d0e6ad4bab&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ìŠ¤ì¼€ì¼ë§ì„ ì™œ í•˜ëŠ”ê±¸ê¹Œ? dot-product ê³„ì‚°ì€ íŠ¹ì„±ìƒ ë¬¸ì¥ì˜ ê¸¸ì´($d$)ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ë” í° ìˆ«ìë¥¼ ê°€ì§€ê²Œ ëœë‹¤. ë¬¸ì œëŠ” ë‚˜ì¤‘ì— softmaxë¥¼ ì·¨í–ˆì„ ë•Œ íŠ¹ì •í•œ ê°’ë§Œ ê³¼ë„í•˜ê²Œ ì‚´ì•„ë‚¨ê³  ë‚˜ë¨¸ì§€ ê°’ë“¤ì€ ì™„ì „íˆ ì£½ì–´ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ì¦‰, **Scaling ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ Gradientë¥¼ ì‚´ë ¤ì•¼ í•˜ëŠ” ê²ƒ**ì´ë‹¤. ê·¸ í›„ Attention Score í–‰ë ¬ì˜ ìœ ì‚¬ë„ë¥¼ 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ normalizeí•˜ê¸° ìœ„í•´ Softmaxë¥¼ ì‚¬ìš©í•œë‹¤. 

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb86c448f-579e-442a-a015-ed3e677dcc98%2FUntitled.png?table=block&id=7108cd1c-7fb3-4350-b767-98e6546d8540&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

Attention Scoreì™€ ì•ì„œ êµ¬í–ˆë˜ Valueë¥¼ ë‚´ì í•˜ë©´ **Self-Attention Value**ë¥¼ êµ¬í•˜ê²Œ ë˜ë©´ì„œ ì „ì²´ì ì¸ Self-Attentionì´ ë§ˆë¬´ë¦¬ ëœë‹¤.

### (2)-4. ì½”ë“œ êµ¬í˜„ (Attention Mechanism)

```python
import torch
import numpy as np
from torch.nn.functional import softmax

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

print(f"Using {device} device\n")
print(f"MPS ì¥ì¹˜ë¥¼ ì§€ì›í•˜ë„ë¡ buildê°€ ë˜ì—ˆëŠ”ê°€? {torch.backends.mps.is_built()}")
print(f"MPS ì¥ì¹˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•œê°€? {torch.backends.mps.is_available()}")

# ============================================================================================
# 1. ë³€ìˆ˜ ì •ì˜
# ============================================================================================
x = torch.tensor([
    [1.0, 0.0, 1.0, 0.0],
    [0.0, 2.0, 0.0, 2.0],
    [1.0, 1.0, 1.0, 1.0],
])

w_query = torch.tensor([
    [1.0, 0.0, 1.0],
    [1.0, 0.0, 0.0],
    [0.0, 0.0, 1.0],
    [0.0, 1.0, 1.0]
])
w_key = torch.tensor([
    [0.0, 0.0, 1.0],
    [1.0, 1.0, 0.0],
    [0.0, 1.0, 0.0],
    [1.0, 1.0, 0.0]
])
w_value = torch.tensor([
    [0.0, 2.0, 0.0],
    [0.0, 3.0, 0.0],
    [1.0, 0.0, 3.0],
    [1.0, 1.0, 0.0]
])

# ============================================================================================
# 2. Q, K, V ë§Œë“¤ê¸°
# ============================================================================================
keys = torch.matmul(x, w_key)
querys = torch.matmul(x, w_query)
values = torch.matmul(x, w_value)

# ============================================================================================
# 3. Attention Score ë§Œë“¤ê¸°
# ============================================================================================
attn_scores = torch.matmul(querys, keys.T)
print(f"Attention Score : {attn_scores}")

# ============================================================================================
# 4. ê° Attention Scoreì— Sqrt(d_K)ë¡œ ë‚˜ëˆ  Softmax ì·¨í•´ì£¼ê¸°
# ============================================================================================
key_dim_sqrt = np.sqrt(keys.shape[-1])
attn_scores_softmax = softmax(attn_scores / key_dim_sqrt, dim=1)
print(f"Attention Score with Softmax : {attn_scores_softmax}")

# ============================================================================================
# 5. Softmaxë¥¼ ì·¨í•´ ì–»ì€ Attention Distributionê³¼ Value Vectorë“¤ì„ ê°€ì¤‘í•©í•˜ì—¬ Attention Valueë¥¼ êµ¬í•˜ê¸°
# ============================================================================================
attn_values = torch.matmul(attn_scores_softmax, values)
print(f"Attention Values : {attn_values}")

for idx, row in enumerate(attn_values):
    print(f"Max prob in {idx}th row : index is {np.argmax(row).item()}, value is {row[np.argmax(row).item()]}")
```

```python
Using mps device

MPS ì¥ì¹˜ë¥¼ ì§€ì›í•˜ë„ë¡ buildê°€ ë˜ì—ˆëŠ”ê°€? True
MPS ì¥ì¹˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•œê°€? True

Attention Score : tensor([[ 2.,  4.,  4.],
                                            [ 4., 16., 12.],
                                            [ 4., 12., 10.]])
Attention Score with Softmax : tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],
                                                                    [8.9045e-04, 9.0884e-01, 9.0267e-02],
                                                                        [7.4449e-03, 7.5471e-01, 2.3785e-01]])
Attention Values : tensor([[1.8639, 6.3194, 1.7042],
                                                [1.9991, 7.8141, 0.2735],
                                                [1.9926, 7.4796, 0.7359]])

Max prob in 0th row : index is 1, value is 6.319371223449707
Max prob in 1th row : index is 1, value is 7.814123153686523
Max prob in 2th row : index is 1, value is 7.479635715484619
```

### (2)-5. Multi-Head Attention

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2e0f9059-0cbe-476a-92bd-a6099a6dd583%2FUntitled.png?table=block&id=6a4516ce-1dde-48dd-8d51-c2b2b3818342&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

**Transformer**ëŠ” ì•ì„œ ì„¤ëª…í•œ Self-Attentionì„ ë³‘ë ¬ë¡œ hë²ˆ í•™ìŠµì‹œí‚¤ëŠ” **Multi-Head Attention** êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. Multi-Headë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì€ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ì„œ ì—¬ëŸ¬ ë¶€ë¶„ì— ë™ì‹œë‹¤ë°œì ìœ¼ë¡œ ì–´í…ì…˜ì„ ê°€í•  ìˆ˜ ìˆì–´ ëª¨ë¸ì´ ì…ë ¥ í† í° ê°„ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¢…ì†ì„±ì„ í¬ì°©í•˜ê³  ë™ì‹œì— ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ì •ë³´ë¥¼ ê²°í•©í•  ìˆ˜ ìˆê²Œ ëœë‹¤. 

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6b9962f1-1ca3-4e89-9a58-35780dba5726%2FUntitled.png?table=block&id=f0fbddf3-8a61-4d4b-9275-142d68072bd0&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì¦‰, Multi-Head Attentionì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê° headëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì„œë¡œ ë‹¤ë¥¸ ë¶€ë¶„ì— ì–´í…ì…˜ì„ ì£¼ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì…ë ¥ í† í°ê°„ì˜ ë” ë³µì¡í•œ ê´€ê³„ë¥¼ ë‹¤ë£° ìˆ˜ ìˆì–´ **í•˜ë‚˜ì˜ ë¬¸ì¥ì´ë¼ í•˜ë”ë¼ë„ ê°ê¸° ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì–´í…ì…˜ì´ ëª¨ì´ê²Œ ë˜ì–´ ë” ë§ì€ ì •ë³´ë¡œ í‘œí˜„ì´ ê°€ëŠ¥**í•˜ë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¢…ì†ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆê³  ë” ì •í™•í•œ ë‹µë³€ì„ ë‚´ëŠ”ë° ë„ì›€ì´ ë˜ë©°, í‘œí˜„ë ¥ì´ í–¥ìƒëœë‹¤.

# â…¢. Residual Learning

## (1). Residual Connection

**Residual Connection**ì€ ResNetì—ì„œ ë“±ì¥í•œ ê°œë…ì´ë‹¤. ê³¼ê±° GoogleNet, VGGë“±ìœ¼ë¡œ Deep CNNì— ëŒ€í•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆìœ¼ë‚˜ ê¹Šì´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ í•™ìŠµì´ ì›í™œí•˜ê²Œ ì˜ ë˜ì§€ ì•Šê³  ìˆìŒì„ í™•ì¸í–ˆì—ˆë‹¤. ë‹¤ì‹œë§í•´ **ë„ˆë¬´ ë³µì¡í•˜ê³  ê¹Šì€ êµ¬ì¡°ë¥¼ ê°€ì§„ ë„¤íŠ¸ì›Œí¬ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒ**ì´ë‹¤. 

ê¸°ë³¸ì ìœ¼ë¡œ Parameter ìˆ«ìê°€ ë§ìœ¼ë©´ ë§ì„ìˆ˜ë¡ Overfittingì´ ì˜ ì¼ì–´ë‚˜ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•œ CVìª½ ëª¨ë¸ì´ ë°”ë¡œ ResNetì´ë¼ê³  í•˜ê³  ì´ì— ì‚¬ìš©ëœ ê°œë…ì´ Residual Connectionê³¼ Residual Learningì´ë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Faf106072-b1d1-4a4c-a3e5-2640dc50e41a%2FUntitled.png?table=block&id=928fb109-2b72-47ea-959a-10f00afc7038&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

**Residual Connection**ì— ëŒ€í•´ ì˜ ì•Œì•„ë³´ê¸°ìœ„í•´ ì ì‹œ ResNet ë‚´ìš©ì„ ê°€ë³ê²Œ ì‚´í´ë³´ì. ResNet ë…¼ë¬¸ì—ì„œ ì œì‹œëœ Residual Connectionì´ ì•ì„œ ì–˜ê¸°í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ë° ì—¬ê¸°ì„œ Residual Connectionì´ ìœ„ìª½ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½ êµ¬ì¡°ì™€ ê°™ì´ ë§ˆì§€ë§‰ í™œì„±í™”í•¨ìˆ˜ $ReLU$ë¥¼ ê±°ì¹˜ê¸° ì „ì— Input Xë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤. ê·¸ì—ë¹„í•´ ê¸°ì¡´ ë°©ì‹(ì™¼ìª½)ì˜ ê²½ìš° Input Xì— ëŒ€í•œ ë‚´ìš©ì´ ì¡´ì¬í•˜ì§€ ì•Šê³  ì˜¤ì§ $\hat y$ë¥¼ í†µí•´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ë°˜ë©´ ResNet ë…¼ë¬¸ì—ì„œ ì œê¸°ëœ êµ¬ì¡°(ì˜¤ë¥¸ìª½)ì˜ ê²½ìš° $\hat y = x + F(x)$ì´ë‹¤. ì´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ Residualì„ ê±°ì¹˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì´ ë‚˜íƒ€ë‚œë‹¤.

$$
x_{l+1} = x_1 + F(x_l)\\x_{l+2} = x_{l+1} + F(x_{l+1}) = x_l + F(x_l) + F(x_{l+1})
$$

ìœ„ì™€ ê°™ì€ êµ¬ì¡°ê°€ ë°˜ë³µë˜ë©´ì„œ ê²°êµ­ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ì™„ì„±ëœë‹¤.

$$
x_L = x_l + \sum_{i=l}^{L-1}F(x_i)
$$

ì¦‰, **íŠ¹ì • ìœ„ì¹˜ì˜ ì¶œë ¥ì€ íŠ¹ì • ìœ„ì¹˜ì—ì„œì˜ ì…ë ¥ê³¼ Residual í•¨ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•´ í•™ìŠµêµ¬ì¡°ê°€ ë‹¨ìˆœí™”ë˜ëŠ” ê²ƒ**ì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì‘ì€ ì”ì°¨ë§Œì„ í•™ìŠµí•˜ëŠ” í•´ë‹¹ ë°©ì‹ì„ **Residual Connection**ì´ë¼ í•˜ëŠ” ê²ƒì´ë‹¤.

## (2). Residual Connections in Transformer

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Febf64c43-9296-4b91-9148-ed84d5fa40da%2FUntitled.png?table=block&id=d740ca58-cf8e-4529-96f7-23d863ff2e31&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ê·¸ë ‡ë‹¤ë©´ **Transformer**ì—ì„œ Residual Learningì´ ì–´ë””ì— ì‚¬ìš©ë ê¹Œ?

ìœ„ ê·¸ë¦¼ì— ë³´ì´ëŠ” ë¹¨ê°„ ì  ìœ„ì¹˜ë§ˆë‹¤ Residual Connectionì´ ì‚¬ìš©ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì•ì„œ ë§í–ˆë“¯ DNNêµ¬ì¡°ê°€ ê¹Šì–´ì§€ê³  ë³µì¡í•´ì§€ë©´ Parameter ìˆ«ìê°€ ë§ì•„ì§€ê³  ì´ëŠ” Overfittingì´ ë  í™•ë¥ ì„ ëŠ˜ë¦°ë‹¤. í•œí¸ NLPë¶„ì•¼ì˜ taskê°€ CVì˜ taskë³´ë‹¤ ë” Gradient Vanishing/Exploding ë˜ê¸° ì‰½ë‹¤ê³  í•œë‹¤. Residual Connectionì€ ì´ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ Transformerì—ë„ ì ê·¹ì±„ìš©ë˜ì—ˆë‹¤. 

**Transformer**ì—ì„œì˜ ***Residual Connection***ì€ í¬ê²Œ 2ê°€ì§€ íš¨ê³¼ë¥¼ ì„ ë³´ì¸ë‹¤.

- **Gradient Vanishing / Explodingì„ ë°©ì§€**í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.
- Attentionì„ ì§„í–‰í•˜ë©° ë‚´ë¶€ì— ì‹ ê²½ë§ì„ íˆ¬ê³¼ì‹œí‚´ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ê¸° ìœ„í•´ Back-Propì„ ìˆ˜í–‰í•˜ê²Œ ë˜ëŠ”ë° ì´ë•Œ ë°œìƒí•˜ëŠ” Positional Encoding ë²¡í„°ì˜ ê°’ì´ í¬ë¯¸í•´ì§„ë‹¤. **Positional Encoding ë²¡í„°ë¥¼ ì†ì‹¤ì—†ì´ ìƒìœ„ ë ˆì´ì–´ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.**

# â…£. Add + Norm

**Normalizatoin**ì€ ì •ê·œí™”ë¥¼ ì˜ë¯¸í•œë‹¤. ìš°ë¦¬ëŠ” ìµìˆ™í•˜ê²Œ Batch Normalizationì€ ë“¤ì–´ë´¤ì–´ë„ Layer Normalizationì€ ìµìˆ™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ì–‘ìª½ ë‹¤ ì‚´í´ë³´ì.

## (1). Batch Normalization

ì‹ ê²½ë§ì— ê° Layerì— ë“¤ì–´ê°€ëŠ” inputì„ batch ë‹¨ìœ„ì˜ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ì •ê·œí™”í•´ í•™ìŠµì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤. ì´ëŸ° ë°©ì‹ì€ Neural Networkì˜ ê° ì¸µë§ˆë‹¤ ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§€ëŠ” í˜„ìƒì„ ì—†ì• ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë‹¨ì ì´ ì¡´ì¬í•˜ëŠ”ë° ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- Mini-batchì˜ í¬ê¸°ì— ì˜ì¡´ì ì´ë‹¤.
- Recurrent based modelì— ì ìš©ì´ ì–´ë µë‹¤.
    - time-stepì˜ ê°œë…ì´ ì ìš©ë˜ê¸° ë•Œë¬¸, ì¦‰ ë§¤ time stepë§ˆë‹¤ ë³„ë„ì˜ í†µê³„ëŸ‰ì´ ì ìš©ë˜ê¸° ë•Œë¬¸

ìš°ë¦¬ëŠ” ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ë¤„ì•¼í•˜ê¸°ì— Batch Normalizationì´ ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ Layer Normalizationì„ ë„ì…í•˜ê²Œ ëœë‹¤.

## (2). Layer Normalization

ì°¸ê³ ìë£Œë¥¼ ì°¾ë‹¤ê°€ ë°œê²¬í•œ Batch Normalizationê³¼ Layer Normalizationì˜ ì°¨ì´ë¥¼ ë…¼ë¬¸ì  ì¸ìš©ì„ ì´ìš©í•´ ì ì€ê²ƒì„ ê°€ì ¸ì™€ ë³´ê² ë‹¤.

- Batch Normalization
    - Estimate the normalization statistics from the summed inputs to the neurons over a mini-batch of training case
- Layer Normalization
    - Estimate the normalization statistics from the summed inputs to the neurons within a hidden layer

ì§ê´€ì ì¸ í‘œí˜„ì´ë¼ ìƒê°í•œë‹¤. ì¦‰, **Layer Normalizationì€ mini-batch ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ inputì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤.**

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe6c52393-d0ad-4804-8ead-cff8303e808b%2FUntitled.png?table=block&id=5448ce48-ed1d-4986-8114-3a4627aa2569&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

# â…¤. Attention in Encoder and Decoder

Transformerì—ëŠ” 3ê°€ì§€ ì¢…ë¥˜ì˜ ì–´í…ì…˜(Attention) ë ˆì´ì–´ê°€ ì‚¬ìš©ëœë‹¤.
ì‚¬ìš©ë˜ëŠ” ì–´í…ì…˜ì€ í•­ìƒ Multi-Head Attentionì´ê³  ì‚¬ìš©ë˜ëŠ” ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.

- **Encoder Self-Attention**
    - ê°ê°ì˜ ë‹¨ì–´ê°€ ì„œë¡œì—ê²Œ ì–´ë– í•œ ì—°ê´€ì„±ì„ ê°€ì§€ëŠ”ì§€ë¥¼ ì–´í…ì…˜ì„ í†µí•´ êµ¬í•˜ë„ë¡ ë§Œë“¤ê³ , ì „ì²´ ë¬¸ì¥ì— ëŒ€í•œ í‘œí˜„ë°©ì‹(=Representation)ì„ í•™ìŠµ í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤.
    
    ![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc0b7e7f5-36c0-4d45-bccd-f2bd80f6df4d%2FUntitled.png?table=block&id=18dd947f-b790-46cb-b5e3-314b0fa2e477&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
    
- **Masked Decoder Self-Attention**
    - ê°ê°ì˜ ë‹¨ì–´ê°€ ì•ìª½ì— ì¶œí˜„í•œ ë‹¨ì–´ë“¤ë§Œì„ ì°¸ê³ í•˜ë„ë¡ ë§Œë“ ë‹¤.
    
    ![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F748ea33e-82c5-48bc-9be5-8b1ddf63696f%2FUntitled.png?table=block&id=7f751141-21b6-47c6-9b95-bb25ddba7d68&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
    
- **Encoder-Decoder Attention**
    - Queryê°€ Decoderì— ìˆê³ , Keyì™€ Valueê°€ Encoderì— ìˆëŠ” Attention êµ¬ì¡°ë¥¼ ì˜ë¯¸í•œë‹¤.
    
    ![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7349a69a-cc07-4563-9a3d-116e17903322%2FUntitled.png?table=block&id=367970a0-445d-42e9-bc79-22107c6af649&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
    

## (1). Masked Multi-Head Attention

Mask(ë§ˆìŠ¤í¬)ë€ ë¬´ì—‡ì„ ì˜ë¯¸í• ê¹Œ?

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3b63e6b0-e505-4b4d-a4cd-d177f5546376%2FUntitled.png?table=block&id=a2107d69-0a32-492f-8953-5827de70c30d&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

Masked í˜¹ì€ Maskingì´ë¼ëŠ” ìš©ì–´ëŠ” í‘œí˜„ ê·¸ëŒ€ë¡œ ë¬´ì–¸ê°€ë¡œ ê°€ë¦°ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤. ì´ ê°œë…ì´ ì™œ ë“±ì¥í–ˆëŠ”ì§€ ë¶€í„° íŒŒì•…í•˜ë„ë¡ í•˜ì.

ê¸°ì¡´ì˜ ì‹œê³„ì—´ ëª¨ë¸êµ¬ì¡°ì˜ ê²½ìš° ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ê°’ì„ ì „ë‹¬ë°›ê¸° ë•Œë¬¸ì— $t+1$ ì‹œì ì˜ ì˜ˆì¸¡ì„ ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ $t$ì‹œì ê¹Œì§€ë¡œ í•œì •ëœë‹¤. **Transformer**ëŠ” ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì§€ ì•Šê³  ì „ì²´ ì…ë ¥ê°’ì„ ë³‘ë ¬ì ìœ¼ë¡œ ì „ë‹¬ë°›ê¸° ë•Œë¬¸ì— ê³¼ê±° ì‹œì ì˜ ì…ë ¥ê°’ì„ ì˜ˆì¸¡í•  ë•Œ ë¯¸ë˜ ì‹œì ì˜ ì…ë ¥ê°’ê¹Œì§€ ì°¸ê³ í•  ìˆ˜ê°€ ìˆë‹¤. ì´ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ê¸°ë²•ë“¤ì„ **Look-ahead Mask**ë¼ê³  í•˜ê³  ì´ê²ƒì„ ì´ìš©í•˜ëŠ” Attentionì„ Masked Attentionì´ë¼ê³  í•œë‹¤. 

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F00c04131-3d60-40c0-9032-bdee106d11c9%2FUntitled.png?table=block&id=3633e376-f8d0-4517-8145-8ac7a3e4f361&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ê·¸ë¦¼ê³¼ ê°™ì´ 3ê°œì˜ ì…ë ¥ì„ ë°›ì€ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. 
Attention Score í–‰ë ¬ì˜ (i,j)ìš”ì†ŒëŠ” ië²ˆì§¸ ì…ë ¥ê°’(Query)ê³¼ jë²ˆì§¸ ì…ë ¥ê°’(Key, Value) ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ì˜ë¯¸í•œë‹¤. ì…ë ¥ê°’ì´ ìˆœì„œë¥¼ ê°€ì§„ ê²½ìš° ië²ˆì§¸ ì…ë ¥ê°’ì€ 1ë¶€í„° iê¹Œì§€ì˜ ê°’ì„ í™œìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê·¸ë¦¼ì—ì„œ ë³´ì—¬ì§€ëŠ” í–‰ë ¬ì˜ ëŒ€ê°ì„  ìœ—ë¶€ë¶„(i<j)ëŠ” ì£¼ì–´ì§„ ì…ë ¥ê°’ì´ ë³¼ ìˆ˜ ì—†ëŠ” ë¯¸ë˜ì‹œì ì˜ ì…ë ¥ê°’ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ ë¶€ë¶„ì„ ê°€ë¦¬ê³  ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ”ê²ƒì´ Masked Attentionì¸ê²ƒì´ë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë³´ë©´ Attention Score í–‰ë ¬ì˜ ëŒ€ê°ì„  ìœ—ë¶€ë¶„ì„ -infë¡œ ë³€ê²½(=Look-ahead Mask)í•œ í›„ Softmaxë¥¼ ì·¨í•´ í•´ë‹¹ ìš”ì†Œë“¤ì˜ Attention Weightë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ Attention Valueë¥¼ ê³„ì‚°í•  ë•Œ ë¯¸ë˜ ì‹œì ì˜ ê°’ì„ ê³ ë ¤í•˜ì§€ ì•Šë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤.

## (2). Encoder-Decoder Attention

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbdae7542-2568-45e2-b14a-611db6ea7072%2FUntitled.png?table=block&id=c42fb03d-ad2d-4107-944e-ce91a2267dd2&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

Masked Self-Attention ì´í›„ì— ë“±ì¥í•˜ëŠ” Attention LayerëŠ” Encoderì˜ ì¶œë ¥ê°’ê³¼ Decoderì˜ ì…ë ¥ê°’ì„ ì´ìš©í•˜ëŠ” Encoder-Decoder Attentionì´ë‹¤. $t$ì‹œì ì˜ ì˜ˆì¸¡ì— ë„ì›€ì´ ë˜ëŠ” Encoderì˜ ì¶œë ¥ê°’ë“¤ë§Œ ì´ìš©í•˜ê³ ì í•˜ëŠ” ê²ƒì´ ëª©ì ìœ¼ë¡œ QueryëŠ” Decoderì˜ Masked Self-Attentionì„ í†µê³¼í•œ ì…ë ¥ê°’ì´, Keyì™€ ValueëŠ” Encoderì˜ ì¶œë ¥ê°’ì´ ëœë‹¤.

# â…¥. Position-wise FeedForward

Postion-wise feedforward NetworkëŠ” Self-Attentionì¸µì„ ê±°ì¹œ í›„ í†µê³¼í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ë§í•œë‹¤. 2ê°œì˜ ì„ í˜•ì¸µì„ ê±°ì¹˜ê²Œ ë˜ëŠ”ë°, ì²«ë²ˆì§¸ ì„ í˜•ì¸µì—ì„œ ReLUë¥¼ ì‚¬ìš©í•œë‹¤. 

# â…¦. Output Probabilities

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fff9f5f87-152e-440d-b22a-6e35fc607270%2FUntitled.png?table=block&id=50671d01-f087-45f8-877d-cf93b4b127d6&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

ì´ë ‡ê²Œ ë‚˜ì˜¨ ê°’ë“¤ì„ Linear Layerì™€ Softmax Layerë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê±°ì¹˜ëŠ”ë°
Linear Layer(FC Layer)ë¥¼ ê±°ì³ Softmaxë¥¼ íƒœìš°ë©´ ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ì°¾ì•„ì•¼í•˜ëŠ” ë‹¨ì–´ì˜ í™•ë¥ ê°’(Softmax Score)ë¥¼ ê°€ì§€ê²Œ ë˜ê³ (vocab_sizeë§Œí¼ì— ëŒ€í•œ ê°ê°ì˜ í™•ë¥ ê°’) ì—¬ê¸°ì„œ argmaxë¡œ ë½‘ì•„ë‚¸ ë‹¨ì–´ë¥¼ ìµœì¢…ë‹¨ì–´ë¡œ ì„ ì •í•˜ê²Œ ëœë‹¤.

![Untitled](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9adb82e4-dc28-49d8-9465-d75730bb9eba%2FUntitled.png?table=block&id=cfb77963-c704-453a-8879-ebaebb3defc0&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)

# â…¦. Transformer : Attention is all you needğŸ”¥

![ì¶œì²˜ : blossominkyungë‹˜ ë¸”ë¡œê·¸, íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒí—¤ì¹˜ê¸° - 1. Positional Encoding](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1392ec82-a566-4b0e-8dba-ddbcff98f259%2FUntitled.png?table=block&id=6fa9865a-8bdc-48fb-9d43-e0ca649ee560&spaceId=ff68afbb-de24-495e-8075-109156ce3ceb&width=2000&userId=d0eae791-c1ef-435a-9af2-1c48d0457a62&cache=v2)
